{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wound-notebook",
   "metadata": {},
   "source": [
    "# [Run Notebook on DeepNote](https://deepnote.com/project/f705a513-f45c-4485-a0d7-f9cbd1260dc8)\n",
    "# Table of Contents:\n",
    "1. [Data](#data)\n",
    "\n",
    "1. [Feature Engineering](#feature_engineering)\n",
    "\n",
    "1. [Algorithms & Search](#alg_search)\n",
    "\n",
    "1. [Evaluation Metrics](#metrics)\n",
    "\n",
    "1. [Ensemble Learning](#ensemble)\n",
    "\n",
    "1. [Final Model Selection and Evaluation](#final_model)\n",
    "\n",
    "1. [Results](#results)\n",
    "\n",
    "1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "going-sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:09:29.504144Z",
     "start_time": "2021-03-12T10:09:28.507215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import  numpy             as np\n",
    "import  pandas            as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import  warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "from    category_encoders          import OneHotEncoder, OrdinalEncoder\n",
    "from    sklearn.experimental       import enable_iterative_imputer\n",
    "from    sklearn.impute             import IterativeImputer, SimpleImputer\n",
    "from    sklearn.base               import TransformerMixin\n",
    "from    sklearn.base               import BaseEstimator\n",
    "from    sklearn.preprocessing      import QuantileTransformer, StandardScaler\n",
    "from    sklearn.compose            import ColumnTransformer\n",
    "\n",
    "# Cross Validation and PipeLine\n",
    "from    sklearn.model_selection    import RandomizedSearchCV, cross_val_score\n",
    "from    sklearn.pipeline           import Pipeline\n",
    "\n",
    "# Models\n",
    "from    sklearn.ensemble           import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier, \\\n",
    "                                          GradientBoostingClassifier, BaggingClassifier, VotingClassifier\n",
    "from    sklearn.neighbors          import KNeighborsClassifier\n",
    "from    sklearn.linear_model       import LogisticRegression, SGDClassifier\n",
    "from    sklearn.svm                import LinearSVC\n",
    "\n",
    "# Model Evaluation\n",
    "from    sklearn.metrics            import balanced_accuracy_score, f1_score, plot_confusion_matrix, precision_score, recall_score,accuracy_score\n",
    "from    sklearn.inspection         import permutation_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-threshold",
   "metadata": {},
   "source": [
    "# Hypothetical Scenario: [Kaggle](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists)\n",
    "____\n",
    "\"A company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n",
    "\n",
    "This dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\"\n",
    "_____\n",
    "# Research Question:\n",
    "_____\n",
    "**Given information about an individual, can we predict whether or not they are currently looking for a new job?**\n",
    "_________\n",
    "\n",
    "\n",
    "# Data <a id='data'></a> \n",
    "_________\n",
    "**Note that our data is imbalanced. 25% of the observations have the target = 1.**\n",
    "\n",
    "The data we will be looking at includes 12 different features:\n",
    "#### Numerical Columns:\n",
    "1. ```city_development_index``` : Float from 0-1\n",
    "2. ```training_hours```: Number of hours trained\n",
    "\n",
    "#### Categorical:\n",
    "1. ```city```: 123 diff cities\n",
    "2. ```gender```: 4 genders: 'Male', 'Female', 'Other', nan\n",
    "1.  ```enrolled_university```: 'no_enrollment', 'Full time course', nan, 'Part time course'\n",
    "2.  ```education_level```: 'Graduate', 'Masters', 'High School', nan, 'Phd', 'Primary School'\n",
    "3. ```company_size```: nan, '50-99', '<10', '10000+', '5000-9999', '1000-4999', '10/49','100-500', '500-999'\n",
    "3. ```relevent_experience```: 'Has relevent experience', 'No relevent experience'\n",
    "6. ```major_discipline```: 'STEM', 'Business Degree', nan, 'Arts', 'Humanities', 'No Major', 'Other'\n",
    "8. ```last_new_job```: nan, 'never', 1,2,3,4, >4\n",
    "7. ```company_type```: 'Early Stage Startup', 'Funded Startup', 'NGO', 'Other', 'Public Sector', 'Pvt Ltd', nan\n",
    "\n",
    "\n",
    "#### Ordinal Categorical:\n",
    "1. ```experience```:  nan, <1, 1 through 20, >20\n",
    "\n",
    "\n",
    "#### Target\n",
    "1. ```target```: 1 = \"Looking for a Job\", 0 = \"Not Looking for a Job\"\n",
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-money",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:09:29.548994Z",
     "start_time": "2021-03-12T10:09:29.507330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19158, 13), (19158,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('HR_Analytics/aug_train.csv')\n",
    "y = df['target'] \n",
    "X = df.drop('target', axis=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "digital-enough",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T11:34:18.107366Z",
     "start_time": "2021-03-12T11:34:18.029469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,6))\n",
    "\n",
    "_, sizes = np.unique(y, return_counts=True)\n",
    "labels = ['Not Looking for Job','Looking for Job']\n",
    "ax.pie(sizes, explode=(0,0.1), labels=labels, autopct='%1.1f%%')\n",
    "ax.set_title('Depiction of Class Imbalances in Target Variable')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-jonathan",
   "metadata": {},
   "source": [
    "# Feature Engineering and Preprocessing Pipeline <a id='feature_engineering'></a>\n",
    "   ____________\n",
    "For this model, we divide our features into 2 main categories: Numerical and Categorical. This is in order to create a preprocessing pipeline that could correctly deal with missing values **in an appropriate way**. \n",
    "- **Categorical Preprocessing:** \n",
    "    1. The first step in this pipeline is to use a **```SimpleImputer```** to fill in the missing values (np.NaN) with \"missing\". Although there are many other strategies to use when filling in missing values, there could be underlying reasons in the data collection why an observation has missing data. Therefore, to simply fill in the missing values with the most_frequent of the data would be adding bias from us, the researcher. Without knowing more about why these values are np.nan, we can just fill in the value with \"missing\" for categorical features. \n",
    "    2. We then pipe this into a **```OneHotEncoder```** in order to encode each variable's values as a separate binary column.\n",
    "    3. Note that after further testing, I've decided to OneHotEncode the ordinal features as well. I did not notice an impact on the model itself when mapping the ordinal features individually to their relative values.\n",
    "    \n",
    "    \n",
    "    \n",
    "- **Numerical Preprocessing:**\n",
    "    1. Instead of using a SimpleImputer, I choose to use an **```IterativeImputer```** instead. The IterativeImputer tries to mimic R's MICE package (Multivariate Imputation by Chained Equations). I decided to use this because I felt that it would be a better solution than the SimpleImputer because:\n",
    "        1. Can't fill in the values with \"missing\" since that affects the pipeline when trying to standardize the columns.\n",
    "        2. [It is a step in dealing with the problem of increased noise due to imputation](https://en.wikipedia.org/wiki/Imputation_(statistics))\n",
    "    2. Next, we use a **```StandardScalar```** to normalize our data. Due to the fact that neither of my numerical columns have any severe outliers, this is preferred over a RobustScalar. In addition, the normalizing helps bring both features within a similar range.\n",
    "    3. Finally, due to the fact that many ML algorithms can perform better when the numerical features have a Gaussian distribution, we use a **```QuantileTransformer```**\n",
    "\n",
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "individual-joining",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:09:29.965837Z",
     "start_time": "2021-03-12T10:09:29.962267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our categorical column we want to OneHotEncoder\n",
    "cat_cols = ['city', 'gender', 'enrolled_university', 'major_discipline', 'experience', 'last_new_job', \n",
    "            'company_type', 'company_size', 'education_level', 'relevent_experience']\n",
    "\n",
    "\n",
    "# Our numerical columns\n",
    "num_cols = ['city_development_index', 'training_hours']\n",
    "\n",
    "\n",
    "# Categorical Pipeline\n",
    "cat_pipe = Pipeline([(\"impute\", SimpleImputer(missing_values=np.nan, fill_value='missing', strategy=\"constant\")),\n",
    "                     (\"encode\", OneHotEncoder())])\n",
    "\n",
    "\n",
    "# Numerical Pipeline\n",
    "num_pipe = Pipeline([(\"impute\", IterativeImputer(missing_values=np.nan, max_iter=10, initial_strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler()),\n",
    "                     (\"transformer\", QuantileTransformer(output_distribution='normal'))])\n",
    "\n",
    "# Group them together with ColumnTransformer\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols),\n",
    "                                   ('numerical', num_pipe, num_cols)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-constitution",
   "metadata": {},
   "source": [
    "# Algorithms & Search <a id='alg_search'></a>\n",
    "_______\n",
    "For this section, I chose a few algorithms to include in my __```RandomizedSearchCV```__:\n",
    "   1. **```RandomForestClassifier```**\n",
    "       1. Why: This was a ML model that I learned in my Intro to ML class. It seemed like a very good contester for this problem because it uses multiple decision trees (that individually tend to overfit on training data) and aggregates their predictions in order to decrease the variance of the model.\n",
    "       2. Hyperparamater Tuning:\n",
    "           - **Min Samples Leaf**: ```np.linspace(1,10,4)```\n",
    "               - Min samples per leaf is a good hyperparameter to help each decision tree make more generalizable predictions. Note that the default is 1, which can lead to decision trees overfitting to the training data.\n",
    "           - **Bootstrap**: ```[True, False]```\n",
    "               - When Bootstrap is True, it means that each decision tree is shown a sample of the training data. This is an attempt to create \"dumber\" decision trees in order to have a better, generalized model. Note that this parameter is dependent on **max_samples** which we set to a small k (between 5 and 20). \n",
    "           - **Class Weight**: ```[None, 'balanced', 'balanced_subsample']```\n",
    "               - Since our data is imbalanced, we want to include a variety of class_weights. Note that since we are not performing SMOTE, I can assume that the class weight will either be balanced or a balanced_subsample. \n",
    "           - **Number of Estimators**: ```np.linspace(25,200,4)```\n",
    "              - Here I choose a variety of different number of estimators. These represent the number of decision trees that make up our \"RandomForest\". The larger the number can potentially lead to decreasing the variance of our model. \n",
    "           - **Max Samples**:  ```np.linspace(0.1, 0.9, 5)```\n",
    "              - This variable is dependent on bootstrap=True. For each base estimator, it will sample between 10-90% of X for training. If bootstrap=False. the whole data will be used to train each tree.\n",
    "           - **Max Features**: ```['auto', 'sqrt', 'log2']```\n",
    "              - For each split in each node, this feature represents the derivation needed to figure out the number of features to take into consideration. We want to make sure it chooses m features where m has a high enough probability to use at least one \"predictive\" feature.\n",
    "         \n",
    "              \n",
    "   2. **```LinearSVC```**\n",
    "       1. Why: Since this is a binary problem (we are predicting 1s and 0s), SVM can perform better than RandomForests (which is intrinsically situated for multi-class problems). In addition, SVM models can also have an advantage to more sparse data, which, with the OneHotEncoder, could prove to be useful. Finally, I decided to use a LinearSVC because it trains much quicker than the traditional SVC.\n",
    "           ![As we can see in the above image, SVC is extremely slow](Images/SGD_Comparison.png)\n",
    "                               - As we can see in the above image, SVC is extremely slow\n",
    "       \n",
    "       2. Hyperparameter Tuning:\n",
    "           - **Class Weight**: ```['balanced', None]```\n",
    "               - Similarly to my decision in RFClassification, I include both 'balanced' and None for the class weight of my SVC model\n",
    "           - **C**: ```np.linspace(0.001, 10, 10)```\n",
    "               - In an attempt to test different regularization weights, I set C to values between 1 and 100.\n",
    "       \n",
    "       \n",
    "   3. **```KNeighborsClassifier```**\n",
    "       1. Why: I included KNN because I felt like it was a fairly different algorithm from the Trees and could be interesting to see how it performs. \n",
    "       2. Hyperparameter Tuning:\n",
    "           - **Number of Neighbors**: ```np.linspace(3, 13, 3)```\n",
    "               - Tweaking the number of neighbors impacts the amount of neighbors we need to look at before classifying our observation.\n",
    "           - **Weights**: ```['uniform', 'distance']```\n",
    "               - Distance: Closer neighbors will have a higher influence on the classification than further neighbors.\n",
    "               - Uniform: Both close and far neighbors have the same weight.\n",
    "           - **p**: ```[1,2]```\n",
    "               - p = 1 -> Manhattan Distance\n",
    "               - p = 2 -> Euclidean distance\n",
    "               \n",
    "               \n",
    "   4. **```ExtraTreesClassifier```**\n",
    "       1. Why: This has a much faster implementation than RandomForestClassifier. This is due to the fact that ExtraTrees chooses its split point threshold's randomly versus RandomForest's more iterative approach.\n",
    "       2. Hyperparameter Tuning:\n",
    "           - This has the same hyperparameter tuning decisions as my RandomForestClassifier\n",
    "           \n",
    "           \n",
    "   5. **```LogisticRegression```**\n",
    "       1. Why: I included logistic regression in order to see how it performs vs. RandomForest. Logistic Regression, in general, is much quicker to train and much easier to interpret than Random Forests.\n",
    "       2. Hyperparameter Tuning:\n",
    "           - **Class Weight**: ```['balanced', None]```\n",
    "               - Similarly to my decision in RFClassification, I include both 'balanced' and None for the class weight of my LogisticRegression model\n",
    "           - **Solver**: ```['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']```\n",
    "               - I included a variety of different solvers as these tend to be data-dependent and it is good to try a variety. Note that since my problem is not a multiclass problem, I am able to include all of these solvers.\n",
    "           - **Penalty**: ```['l1', 'l2', 'elasticnet', 'none']```\n",
    "               - This represents the penalty used in my loss function.\n",
    "               \n",
    "_______\n",
    "\n",
    "# Evaluation Metrics <a id='metrics'></a>\n",
    "__________\n",
    "I decided to use a **weighted f1 score** as my metric since my data is imbalanced.  It is equally important for this business to correctly predict those looking for work (Precision) while maintaining a high Recall Rate (we aren't missing potential job-seeking individuals). This priority is due to a focus on resource allocation. If a business wants to reach out to individuals that are currently looking for jobs, we want to use a model that efficiently allocates their resources (recruiters) by avoiding reaching out to False Positives and making sure we don't miss any True Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-disaster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:09:29.970481Z",
     "start_time": "2021-03-12T10:09:29.968489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper class\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dependent-implement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:12:03.580734Z",
     "start_time": "2021-03-12T10:09:29.972302Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)  # Use a random seed to have consistent results\n",
    "\n",
    "pipe = Pipeline([(\"preprocessing\", preprocessing),\n",
    "                 (\"clf\", DummyEstimator())])\n",
    "\n",
    "search_space = [\n",
    "            {'clf': [ExtraTreesClassifier(n_jobs=-1)], \n",
    "                'clf__min_samples_leaf': np.linspace(1, 30, 4, dtype=int),\n",
    "                'clf__bootstrap': [True, False],\n",
    "                'clf__max_samples': np.linspace(5, 30, 4, dtype=int), \n",
    "                'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                'clf__n_estimators': np.linspace(25, 200, 4, dtype=int)},\n",
    "                \n",
    "            {'clf': [RandomForestClassifier(n_jobs=-1)], \n",
    "                'clf__min_samples_leaf': np.linspace(1, 10, 4, dtype=int),\n",
    "                'clf__bootstrap': [True, False],\n",
    "                'clf__max_samples': np.linspace(0.1, 0.9, 5),\n",
    "                'clf__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                'clf__n_estimators': np.linspace(25, 200, 4, dtype=int)},\n",
    "               \n",
    "            {'clf': [KNeighborsClassifier(n_jobs=-1)],\n",
    "                'clf__n_neighbors': np.linspace(3, 13, 3, dtype=int),\n",
    "                'clf__weights': ['uniform', 'distance'],\n",
    "                'clf__p': [1,2]},\n",
    "               \n",
    "            {'clf': [LinearSVC()],  \n",
    "                'clf__class_weight': ['balanced', None],\n",
    "                'clf__C': np.linspace(0.001, 10, 10)},\n",
    "               \n",
    "            {'clf': [LogisticRegression(n_jobs=-1)],  \n",
    "                'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                'clf__class_weight': ['balanced', None],\n",
    "                'clf__penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "\n",
    "            {'clf': [SGDClassifier(n_jobs=-1)],  \n",
    "                'clf__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                'clf__class_weight': ['balanced', None],\n",
    "                'clf__penalty': ['l1', 'l2', 'elasticnet']}]\n",
    "\n",
    "# Create grid search \n",
    "gs = RandomizedSearchCV(pipe, \n",
    "                        search_space, \n",
    "                        scoring='f1_weighted',\n",
    "                        n_iter=50,\n",
    "                        cv=5,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "gs.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imperial-woman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:12:03.587087Z",
     "start_time": "2021-03-12T10:12:03.582634Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Weighted F1 Score: 0.797\n",
      "\n",
      "Best Model: RandomForestClassifier(bootstrap=False, class_weight='balanced_subsample',\n",
      "                       max_samples=0.1, min_samples_leaf=4, n_estimators=83,\n",
      "                       n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Weighted F1 Score: {gs.best_score_:.3f}\\n\\nBest Model: {gs.best_params_[\"clf\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-india",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV Results: \n",
    "Best Model:\n",
    "```python\n",
    "# Before\n",
    "RandomForestClassifier(bootstrap=False,  # No bootstrapping\n",
    "                       max_samples=0.1,   # Percent of samples to be trained on\n",
    "                       class_weight='balanced_subsample',  # Good because our data is imbalanced\n",
    "                       min_samples_leaf=4,  # Increasing can improve generalizability if we are overfitting\n",
    "                       n_estimators=83,  # Plenty of trees to train\n",
    "                       n_jobs=-1)\n",
    "\n",
    "```\n",
    "Weighted F1 Score: ``` 0.797```\n",
    "\n",
    "**Note**:\n",
    "- We notice that bootstrapping let to a worse cross-validated Weighted F1 score.\n",
    "- In addition, with bootstrapping=False, that means that class_weight being 'balanced_subsample' is the same as 'balanced'. It also means that we are not actually using our max_samples parameter and can remove it.\n",
    "- In addition, our min_samples_leaf was 4, which helps weaken the individual decision trees, but increase model generalizability (decreases variance)\n",
    "- The class_weight being balanced which makes sense because our data is imbalanced and the model needs to add weights to the different labels. This can lead to a higher recall score and a lower precision score.\n",
    "   \n",
    "```python\n",
    "# After\n",
    "RandomForestClassifier(bootstrap=False,  # No bootstrapping\n",
    "                       class_weight='balanced',  # Good because our data is imbalanced\n",
    "                       min_samples_leaf=4,  # Increasing can improve generalizability if we are overfitting\n",
    "                       n_estimators=83,  # Plenty of trees to train\n",
    "                       n_jobs=-1)\n",
    "\n",
    "```\n",
    "\n",
    "_________\n",
    "# Ensemble Learning <a id='ensemble'></a>\n",
    "_______\n",
    "In order to further improve our model, we can look into ensemble learning as a way to improve our metric (f1_weighted). Note that we will be primarily looking at:\n",
    "1. VotingClassifier()\n",
    "    - VotingClassifier combines multiple machine learning models and takes the most common prediction (\"Hard Voting\") or the probability-weighted average of the individual learners (\"Soft Voting\").\n",
    "2. Bagging\n",
    "    - Bagging is useful because it tends to reduce the time of each individual model and improve the overall generality. In addition, it can be used with a variety of different models.\n",
    "3. Boosting\n",
    "    - Boosting has shown a lot of promise in improving model metrics the most (at least what I've read online). Therefore, I predict that this may lead to my best evaluation metric.\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fossil-watson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:22:08.074209Z",
     "start_time": "2021-03-12T10:21:44.857374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted F1 -> Mean: 0.796  |  Std: 0.006\n",
      "recall      -> Mean: 0.733  |  Std: 0.004\n",
      "precision   -> Mean: 0.557  |  Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#Helper Function for displaying results of cross_val_score\n",
    "def print_val_score(res, name):\n",
    "    mean   = np.mean(res)\n",
    "    std    = np.std(res)\n",
    "    print(f\"{name:<11} -> Mean: {mean:.3f}  |  Std: {std:.3f}\")\n",
    "    \n",
    "    \n",
    "# Calculate the general stats for our first model to have a way to compare other methods\n",
    "rfc_clf =  RandomForestClassifier(bootstrap=False, \n",
    "                                  class_weight='balanced',\n",
    "                                  min_samples_leaf=4, \n",
    "                                  n_estimators=83,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', rfc_clf)])\n",
    "\n",
    "scores = []\n",
    "\n",
    "scores.append(cross_val_score(pipe, X, y, cv=5, scoring='f1_weighted'))\n",
    "scores.append(cross_val_score(pipe, X, y, cv=5, scoring='recall'))\n",
    "scores.append(cross_val_score(pipe, X, y, cv=5, scoring='precision'))\n",
    "\n",
    "names = ['weighted F1', 'recall', 'precision']\n",
    "for score, name in zip(scores, names):\n",
    "    print_val_score(score, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-drilling",
   "metadata": {},
   "source": [
    "# Voting Classifier:\n",
    "_______\n",
    "In this section, I simply combine 4 different models:\n",
    "1. My best model from my previous randomizedSearchCV\n",
    "2. A Logistic Regression model\n",
    "3. An ExtraTreesClassifier\n",
    "   - Note that the top two models were chosen through repeated iterations of my randomizedSearchCV where I removed the best performer at each step to see the \"next best model\".\n",
    "4. A bootstrapped RandomForestClassifier (A weaker version of my first model, but can maybe help generalize more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abandoned-mitchell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:23:57.283035Z",
     "start_time": "2021-03-12T10:22:53.975852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 -> Mean: 0.794  |  Std: 0.006\n",
      "recall      -> Mean: 0.742  |  Std: 0.005\n",
      "precision   -> Mean: 0.549  |  Std: 0.010\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Different Models that I will be using:\n",
    "rfc_clf =  RandomForestClassifier(bootstrap=False, \n",
    "                                  class_weight='balanced',\n",
    "                                  min_samples_leaf=4, \n",
    "                                  n_estimators=83,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "lr_clf  = LogisticRegression(class_weight='balanced', \n",
    "                             n_jobs=-1, \n",
    "                             solver='sag')\n",
    "\n",
    "etc_clf = ExtraTreesClassifier(class_weight='balanced', \n",
    "                               max_samples=20,\n",
    "                               min_samples_leaf=10, \n",
    "                               n_estimators=50, \n",
    "                               n_jobs=-1)\n",
    "\n",
    "\n",
    "rfc_clf_2 = RandomForestClassifier(bootstrap=True, \n",
    "                                   max_samples=0.63,\n",
    "                                   class_weight='balanced',\n",
    "                                   min_samples_leaf=10, \n",
    "                                   n_estimators=50,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "\n",
    "# Different pipelines\n",
    "pipe1 = Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', rfc_clf)])\n",
    "\n",
    "pipe2 = Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', lr_clf)])\n",
    "\n",
    "pipe3= Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', etc_clf)])\n",
    "\n",
    "pipe4 = Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', rfc_clf_2)])\n",
    "\n",
    "estimators = [('rfc', pipe1),\n",
    "              ('lr', pipe2),\n",
    "              ('etc', pipe3),\n",
    "             ('rtc_2', pipe4)]\n",
    "\n",
    "# Initialize my VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators = estimators,\n",
    "                                  voting = 'soft', \n",
    "                              weights=[2/5, 1/5, 1/5, 1/5],\n",
    "                             n_jobs=-1)\n",
    "\n",
    "# Use Cross_val_score to see how well my model performs:\n",
    "scores = []\n",
    "\n",
    "scores.append(cross_val_score(voting_clf, X, y, cv=5, scoring='f1_weighted'))\n",
    "scores.append(cross_val_score(voting_clf, X, y, cv=5, scoring='recall'))\n",
    "scores.append(cross_val_score(voting_clf, X, y, cv=5, scoring='precision'))\n",
    "\n",
    "names = ['Weighted F1', 'recall', 'precision']\n",
    "for score, name in zip(scores, names):\n",
    "    print_val_score(score, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-conjunction",
   "metadata": {},
   "source": [
    "## VotingClassifier Results:\n",
    "    - As we can see, our weighted F1 score is pretty close to how our single RandomForestClassifier (RFC) performed.\n",
    "    - Even when comparing the Precision and Recall scores, we see that neither model (VotingClassifier or RFC) were clear \"winners\".\n",
    "    - Note that including a larger and more diverse amount of estimators could greatly improve this model.\n",
    "_________\n",
    "# Bagging:\n",
    "___________\n",
    "In this section, I experiment the use of bagging with my RandomForestClassifier parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "express-biotechnology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:25:32.596876Z",
     "start_time": "2021-03-12T10:23:57.285066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 -> Mean: 0.797  |  Std: 0.006\n",
      "recall      -> Mean: 0.734  |  Std: 0.006\n",
      "precision   -> Mean: 0.559  |  Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "bag = BaggingClassifier(rfc_clf,\n",
    "                        n_estimators=11,\n",
    "                        max_samples=0.8,\n",
    "                        max_features=0.9,\n",
    "                        oob_score=True)\n",
    "\n",
    "bagging_pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                         ('model', bag)])\n",
    "\n",
    "# Use Cross_val_score to see how well my model performs:\n",
    "scores = []\n",
    "\n",
    "scores.append(cross_val_score(bagging_pipe, X, y, cv=5, scoring='f1_weighted'))\n",
    "scores.append(cross_val_score(bagging_pipe, X, y, cv=5, scoring='recall'))\n",
    "scores.append(cross_val_score(bagging_pipe, X, y, cv=5, scoring='precision'))\n",
    "\n",
    "names = ['Weighted F1', 'recall', 'precision']\n",
    "for score, name in zip(scores, names):\n",
    "    print_val_score(score, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-spending",
   "metadata": {},
   "source": [
    "## Bagging Results:\n",
    "    - Similarly to VotingClassifier, the Bagging did not substantially improve the model's performance, nor did it decrease the model's standard deviation. I am sure with more hyper-parameter tuning I can improve upon this model.\n",
    "    \n",
    "_______\n",
    "# Boosting\n",
    "In this section, will look at implementing a gradient boosting machine, specifically using a GradientBoostingClassifier as my final estimator in my StackingClassifier.\n",
    "\n",
    "__________\n",
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "severe-chance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:29:24.491761Z",
     "start_time": "2021-03-12T10:25:32.598821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted -> Mean: 0.801  |  Std: 0.005\n",
      "recall      -> Mean: 0.641  |  Std: 0.009\n",
      "precision   -> Mean: 0.589  |  Std: 0.014\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "final_estimator = GradientBoostingClassifier()\n",
    "\n",
    "estimators = [('rfc', pipe1),\n",
    "              ('lr', pipe2),\n",
    "              ('etc', pipe3),\n",
    "              ('rfc2', pipe4)]\n",
    "\n",
    "\n",
    "boosted_stack = StackingClassifier(estimators=estimators,\n",
    "                        final_estimator=final_estimator,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Use Cross_val_score to see how well my model performs:\n",
    "scores = []\n",
    "\n",
    "scores.append(cross_val_score(boosted_stack, X, y, cv=5, scoring='f1_weighted'))\n",
    "scores.append(cross_val_score(boosted_stack, X, y, cv=5, scoring='recall'))\n",
    "scores.append(cross_val_score(boosted_stack, X, y, cv=5, error_score='raise', scoring='precision'))\n",
    "\n",
    "names = ['f1_weighted', 'recall', 'precision']\n",
    "for score, name in zip(scores, names):\n",
    "    print_val_score(score, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-durham",
   "metadata": {},
   "source": [
    "## Ensemble Model Results:\n",
    "\n",
    "1. VotingClassifier():\n",
    "    - Using RandomForestClassifier, LogisticRegression, and ExtraTreesClassifier, this strategy yielded similar results to my initial RandomForestClassifier.\n",
    "    - Note that I used a 'soft' scoring since every one of my models include probabilities to serve as a metric of confidence in a prediction.\n",
    "    - I decided to not use this method for my final model because it did not perform significantly better than my initial model.\n",
    "    \n",
    "    \n",
    "2. Bagging:\n",
    "    - Similarly, the results of using a BaggingClassifier was fairly similar to the simple RandomForestClassifier.\n",
    "    - I decided to not use this model for my final model.\n",
    "\n",
    "\n",
    "3. GradientBoostingClassifier():\n",
    "    - This strategy, using a GradientBoostingClassifier as my final_estimator in a StackingClassifier gave me my best Weighted F1 score. That being said, its recall was almost 10% less than the other estimators with a marginally better precision score. \n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-invention",
   "metadata": {},
   "source": [
    "# Final Model Selection <a id='final_model'></a>\n",
    "___________\n",
    "Although I briefly looked at some ensemble techniques, I decided to stick with my original RandomForestClassifier **(RFC)** because of it's simultaneously high F! weighted score and recall. In addition, the RFC, compared to the ensemble techniques trained much quicker.\n",
    "_______\n",
    "## Specifications of the Final Model:\n",
    "\n",
    "\n",
    "Our final model is a **RandomForestClassifier** with the following hyperparameters:\n",
    "\n",
    "```python\n",
    "RandomForestClassifier(bootstrap=False,  # No bootstrapping\n",
    "                       class_weight='balanced',  # Good because our data is imbalanced\n",
    "                       min_samples_leaf=4,  # Increasing can improve generalizability if we are overfitting\n",
    "                       n_estimators=83,  # Plenty of trees to train\n",
    "                       n_jobs=-1)\n",
    "\n",
    "```\n",
    "\n",
    "    - Note that since bootstrap=False, this model is training 83 decision trees with all the available data.\n",
    "_________\n",
    "\n",
    "**But wait, there's more!**\n",
    "In order to fit our RandomForestClassifier on the data, we needed to first include our preprocessing steps.\n",
    "\n",
    "This involved:\n",
    "   1. Using OneHotEncoding on all categorical features after imputing \"missing\" values anywhere the data is not collected.\n",
    "   \n",
    "```python\n",
    "cat_pipe = Pipeline([(\"impute\", SimpleImputer(missing_values=np.nan, \n",
    "                                              fill_value='missing', \n",
    "                                              strategy=\"constant\")),\n",
    "                     (\"encode\", OneHotEncoder())])\n",
    "\n",
    "```\n",
    "   2. Use IterativeImputer for the numerical values, scale them, and then change their distribution into a Gaussian one.\n",
    "   \n",
    "```python\n",
    "num_pipe = Pipeline([(\"impute\", IterativeImputer(missing_values=np.nan, \n",
    "                                                 max_iter=10, \n",
    "                                                 initial_strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler()),\n",
    "                     (\"transformer\", QuantileTransformer(output_distribution='normal'))])\n",
    "\n",
    "```\n",
    "   3. Grouping them together with a ColumnTransformer().\n",
    "```python\n",
    "preprocessing = ColumnTransformer([(\"categorical\", cat_pipe, cat_cols),\n",
    "                                   (\"numerical\", num_pipe, num_cols)]) \n",
    "\n",
    "\n",
    "_____\n",
    "## Evaluating the Final Model <a id='results'></a>\n",
    "Now that we have a final model, we can:\n",
    "   1. Predict on the test set and see what our Weighted F1 score is.\n",
    "   2. Look at the confusion matrix of our predictions as well.\n",
    "   3. Look at some feature importance since we are using a Tree-based algorithm.\n",
    "_______\n",
    "**First**, we need to load in our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "collect-associate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:36:51.270906Z",
     "start_time": "2021-03-12T10:36:51.258285Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('HR_Analytics/aug_test.csv')\n",
    "y_test = np.load('HR_Analytics/aug_test_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-status",
   "metadata": {},
   "source": [
    "### 1. Train our model on all of the training data and then see how it performs on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "smart-ecology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:36:56.042600Z",
     "start_time": "2021-03-12T10:36:54.617515Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)  # Consistent results\n",
    "\n",
    "# Final Model\n",
    "final_model =  RandomForestClassifier(bootstrap=False, \n",
    "                                  class_weight='balanced',\n",
    "                                  min_samples_leaf=4, \n",
    "                                  n_estimators=83,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "\n",
    "# Different pipelines\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                  ('model', final_model)])\n",
    "\n",
    "pipe.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "african-briefs",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:36:57.593662Z",
     "start_time": "2021-03-12T10:36:57.495739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted f1_Score = 0.796\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "print(f\"Weighted f1_Score = {f1_score(y_test, y_pred, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-chocolate",
   "metadata": {},
   "source": [
    "**Result**: As we can see, our model has a **weighted f1-score of 0.796** on our testing set, which is around the same as the training set. This is a good sign because it can imply that our model might not have been overfit on the training and that it generalizes on new data just as well as data it was trained on. It also implies that our test and train data most likely come from the same distribution.\n",
    "__________\n",
    "### 2. View the confusion matrix of our predictions and calculate the accuracy, recall and precision scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "welsh-dakota",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:37:39.386740Z",
     "start_time": "2021-03-12T10:37:39.178629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.72  |  Precision Score: 0.59  |  f1 Weighted Score: 0.80\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "plot_confusion_matrix(pipe, \n",
    "                      X_test, \n",
    "                      y_test,\n",
    "                      display_labels=['Not Looking', 'Looking for Work']);\n",
    "\n",
    "\n",
    "rec = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Recall Score: {rec:.2f}  |  Precision Score: {prec:.2f}\",\\\n",
    "     f\" |  f1 Weighted Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-fruit",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "1. Looking at the confusion matrix, we see that our model did fairly well. In particular, our model misclassified only 18% of individuals not looking for work, and 27% of individuals looking for work.\n",
    "\n",
    "\n",
    "2. In particular, our precision score was 59%, which means that **\"When my model predicted someone to be Looking for Work, it was accurate 59% of the time.\"**. Although this isn't an amazing score, it can still be useful for a variety of potential business cases. In the narrative that we are using this model to help better allocate the time of our recruiters, having a 59% chance that every candidate the recruiter is talking to is looking for a job, can be a huge time saver. This is compared to not using the model and having only a 25% chance of reaching out to someone at random and them currently looking for work.\n",
    "\n",
    "\n",
    "3. In addition, we can look at the recall score. At 72%, this means that **\"When an individual was looking for work, our model accurately classified them 72% of the time.\"** This is also an important metric for the above business situation. In particular, we would want to minimize our False Negatives, aka: when our model inaccurately predicts someone who is looking for work. For the business, they may want a high recall in order to avoid missing out on good candidates.\n",
    "\n",
    "\n",
    "4. All together, we can look at the weighted f1 score of **80%** because our business case revolves around maximizing both precision and recall. Note that since this is an imbalanced dataset, our f1 score calculates the metric for each label, weighs it proportionally to its relative frequency, and outputs a score that, in this case, is not in-between the recall and precision score. \n",
    "______\n",
    "### 3. Feature Importance.\n",
    "For interpretability, it is always important to try to extract feature importance from a model. In other words, what features were deemed the \"most important\" for our model. For this task, I use sklearn's built-in permutation_importance over 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "oriented-combine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T10:58:43.301754Z",
     "start_time": "2021-03-12T10:57:06.346642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature           Mean       std     \n",
      "----------------------------------------------\n",
      "city_development_index    0.09  +/- 0.01\n",
      "city                      0.04  +/- 0.00\n",
      "company_size              0.02  +/- 0.00\n",
      "major_discipline          0.02  +/- 0.00\n",
      "education_level           0.01  +/- 0.00\n",
      "company_type              0.01  +/- 0.00\n",
      "last_new_job              0.01  +/- 0.00\n",
      "relevent_experience       0.00  +/- 0.00\n",
      "enrolled_university       0.00  +/- 0.00\n",
      "gender                    0.00  +/- 0.00\n",
      "enrollee_id               0.00  +/- 0.00\n",
      "experience                -0.00  +/- 0.00\n",
      "training_hours            -0.00  +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "r = permutation_importance(pipe, X_test, y_test,\n",
    "                           n_repeats=100)\n",
    "\n",
    "'''\n",
    "Code Source: https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "'''\n",
    "print(f\"{'Feature':^25}{'Mean':^8}{'std':^13}\")\n",
    "print('-'*(25+8+13))\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(f\"{X.columns[i]:<25}\",\\\n",
    "          f\"{r.importances_mean[i]:.2f}\",\\\n",
    "          f\" +/- {r.importances_std[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-playback",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "1. Interestingly enough, it looks like the city_development_index is the most impactful when making this prediction. In order to better understand the relationship between city_development_index and the target variable, it is helpful to look at a quick plot:\n",
    "\n",
    "<img src=\"Images/EDA_City_Development_Index.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "2. As we can see, it looks like as the city_development_index increases, people are less likely to be looking for a job. This makes sense as a higher city development index could mean that people are more content with their current location. Note that this assumes that looking for work tends to be a factor of location as well as things related to your current job (job type, company size, etc). In addition, this graph could be interpreted on the flip-side. People in cities with a low development index are much more likely to be seeking a new job.\n",
    "\n",
    "3. Besides city_development_index, the city, company_size and major_discipline also had a relatively high impact.\n",
    "\n",
    "_________\n",
    "# Conclusion: <a id='conclusion'></a>  \n",
    "\n",
    "## Summary:\n",
    "With a goal of implementing a variety of skills learned in my Machine Learning Lab, this project focused on HR data to help identify whether or not someone is currently looking for a job. After extensive EDA, I decided to construct three separate pipelines for preprocessing: one for each type of feature (numerical, categorical, ordinal). Next, although my data was imbalanced, I decided against using oversampling with SMOTE because it didn't lead to a noticeable improvement (as shown with cross validation). Finally, I tried several of ensemble techniques and decided that my final model would be the original RandomForestClassifier that I found in my RandomizedSearchCV. Note that I used a weighted f1 score as my metric to compare models with as it equally values precision and recall scores while taking into consideration that the data is imbalanced.\n",
    "\n",
    "## Common Questions:\n",
    "1. **Why does any of this matter?**\n",
    "    - **I am glad you asked!** Although this project was primarily as a tool to explore different modeling methods on fairly clean data, we did it in a way focusing on a hypothetical scenario where this data would be used. This is helpful when working on actual business problems, because it is important to iterative over different models and have a consistent and **relevant** metric that we compare each model with. This metric is relevant to the business use-case.\n",
    "\n",
    "\n",
    "2. **Why did I impute categorical variables to be \"missing\" instead of \"most_frequent\"?**\n",
    "   - I chose to do this because imputing the most_frequent can add bias to our model. In essence, we are assuming that the entry was empty because of a clerical error, which could, in itself, have an impact on our target variable. One could argue that me imputing \"missing\" could be adding my own bias that these weren't just clerical errors and have an equally negative impact on our model. In the end, it was personal preference.\n",
    "   - **Follow up: Why did I not impute missing for numerical as well?**\n",
    "       - With numerical data, we need slightly different imputing strategies. This is because we want our end result to be all numerical. Therefore, I chose to use an IterativeImputer which I explain below.\n",
    "\n",
    "\n",
    "\n",
    "3. **Why did I use an IterativeImputer for my numerical data?**\n",
    "    - This decision was made primarily because I did not want to have a single rule of only imputing the median. Instead, an IterativeImputer works in the following way:\n",
    "        - Say you have 4 columns ('a', 'b', 'c', 'd') and one column ('d') is missing some values. An iterative imputer will train a new model trying to predict the missing values in 'd' with the values in ('a', 'b', 'c').\n",
    "\n",
    "\n",
    "5. **Why did I use a QuantileTransformer on my numerical data?**\n",
    "    - [\"Many machine learning algorithms prefer or perform better when numerical variables have a Gaussian or standard probability distribution.\"](https://machinelearningmastery.com/quantile-transforms-for-machine-learning/). In particular, I included a LogisticRegression model in my RandomizedSearchCV which assumes that the data is normalized.\n",
    "\n",
    "\n",
    "6. **Why did I use cross_val_score?**\n",
    "    - I decided to use cross_val_score because I felt that it was a better route of getting a good sense of how well a model performs in order to properly compare it to other modeling strategies. In addition, it was a quick way to compare different metrics like Recall and Precision to visualize the trade-off between models.\n",
    "    \n",
    "    \n",
    "7. **Why did neither of my Ensemble Techniques lead to a significantly better model?**\n",
    "   - A lot of this has to do with fine tuning these models. For example:\n",
    "       - The VotingClassifier could potentially do much better if I spend more time deciding which (and how many) models to include. In addition, there are hyper-parameters that come with this model that could also be tweaked within a RandomizedSearchCV.\n",
    "       - The BaggingClassifier could also be tweaked more since I did not experiment iteratively through different parameter ranges.\n",
    "       - The Boosting probably has the most potential for success, but again, it involves extensive tweaking of hyper parameters and model selection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Future Steps\n",
    "1. As Boosting has taken Kaggle competitions by stride, it would be interesting to see more exploration on how boosting can further improve this model. In particular, I am interested to see how XGBoost and CatBoost can be used to solve this problem.\n",
    "2. Further exploration on other feature engineering techniques that could improve predictability.\n",
    "3. Exploring other approaches for counteracting imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
